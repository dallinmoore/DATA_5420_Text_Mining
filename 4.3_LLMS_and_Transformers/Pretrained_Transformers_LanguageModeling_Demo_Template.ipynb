{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9ea6a735",
      "metadata": {
        "id": "9ea6a735"
      },
      "source": [
        "# Pretrained Transformer Models from Hugging Face\n",
        "\n",
        "AHugging Face is an AI company that develops and hosts tools used to develop machine learning based application. Amongst those tools is a transformers library accessible through Python that are pretrained for a variety of NLP tasks and fine-tuned to suit specific types of text data. These models are open-source and can be accessed throguh the `transformers` library in Python.\n",
        "\n",
        "In this demonstration, I will run through several of these transformer models which have been pretrained for:\n",
        "\n",
        "* Sentiment Analysis\n",
        "* Abstraction-based summarization\n",
        "* Question-Answering\n",
        "* Table Question-Answering\n",
        "\n",
        "But you can find a larger list of models, organized by task at the [Hugging Face website](https://huggingface.co/models). Let's begin by doing a pip install of `transformers` and then loading the remaining dependencies. Note that downloading these models can take a few minutes, depending on connection, but this only needs to occur once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26eb1c11",
      "metadata": {
        "id": "26eb1c11"
      },
      "outputs": [],
      "source": [
        "# pip install transformers\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import pipeline,TapexTokenizer, BartForConditionalGeneration\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1355edb",
      "metadata": {
        "id": "d1355edb"
      },
      "source": [
        "## Text Classification - Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d17caa36",
      "metadata": {
        "id": "d17caa36"
      },
      "outputs": [],
      "source": [
        "# Using BERT for sentiment analysis\n",
        "sentiment_model = pipeline('sentiment-analysis', model=\"nlptown/bert-base-multilingual-uncased-sentiment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e101b8b",
      "metadata": {
        "id": "5e101b8b"
      },
      "outputs": [],
      "source": [
        "fin = pd.read_csv(, nrows = )\n",
        "# display heading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29eb84ad",
      "metadata": {
        "id": "29eb84ad"
      },
      "outputs": [],
      "source": [
        "def truncate_sequence(,): # add inputs and comments\n",
        "    if len(sequence) > max_length:\n",
        "        sequence = sequence[:max_length]\n",
        "    return sequence\n",
        "\n",
        "# Apply function to Sentence column & apply sentiment model\n",
        "fin[''] = fin[''].apply(lambda x: )[0]['label'])\n",
        "\n",
        "\n",
        "# Print the sentiment for each article\n",
        "fin.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebd40b22",
      "metadata": {
        "id": "ebd40b22"
      },
      "outputs": [],
      "source": [
        "# examine some sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36f7dde3",
      "metadata": {
        "id": "36f7dde3"
      },
      "source": [
        "## Text Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae96b94a",
      "metadata": {
        "id": "ae96b94a"
      },
      "outputs": [],
      "source": [
        "# Load transcript from file\n",
        "transcript_file = 'Analytics Training - Text Analysis with Carly Fox_2023-06-15.txt'\n",
        "with open(transcript_file, 'r') as file:\n",
        "    transcript_text = file.read()\n",
        "\n",
        "\n",
        "def filter_speaker(text, target_speaker):\n",
        "    filtered_lines = [] # create empty list of filtered lines\n",
        "    lines = text.split('\\n') # split lines where there are new lines\n",
        "    i = 0\n",
        "    while i < len(lines): # when lines are longer than 0\n",
        "        speaker = lines[i].strip() # remove extra whitespaces\n",
        "        if speaker.startswith(target_speaker): # if the line begins with the target speaker\n",
        "            dialogue = lines[i+1].strip() # save as dialogue\n",
        "            if dialogue:\n",
        "                filtered_lines.append(dialogue) # add dialogue to filtered_lines\n",
        "        i += 2  # Skip to the next speaker line\n",
        "    return '\\n'.join(filtered_lines) # rejoin filtered_lines\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Remove punctuation and convert to lowercase\n",
        "    #cleaned_sentences = [re.sub(r'[^\\w\\s]', '', sentence.lower()) for sentence in sentences]\n",
        "\n",
        "    # Remove stopwords and empty sentences\n",
        "    #stop_words = set(stopwords.words('english'))\n",
        "    #cleaned_sentences = [sentence for sentence in cleaned_sentences if sentence and sentence not in stop_words]\n",
        "\n",
        "    return cleaned_sentences\n",
        "\n",
        "target_speaker = 'Carly Fox'\n",
        "filtered_text = filter_speaker(transcript_text, target_speaker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "168d003e",
      "metadata": {
        "id": "168d003e"
      },
      "outputs": [],
      "source": [
        "def truncate_text(): # add inputs and comments\n",
        "    tokens = text.split()\n",
        "    if len(tokens) > max_tokens:\n",
        "        tokens = tokens[:max_tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"knkarthick/MEETING_SUMMARY\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shortened_text =\n",
        "# print the shorted text"
      ],
      "metadata": {
        "id": "YJa6jhz4rPRI"
      },
      "id": "YJa6jhz4rPRI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eb264c8",
      "metadata": {
        "id": "4eb264c8"
      },
      "outputs": [],
      "source": [
        "# set length of summary and apply to shortened text\n",
        "summary =\n",
        "\n",
        "# print the summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f3f0530",
      "metadata": {
        "id": "0f3f0530"
      },
      "source": [
        "## Question-Answering System"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the QA pipeline\n",
        "qa_pipeline = pipeline('question-answering', model='distilbert-base-uncased-distilled-squad')"
      ],
      "metadata": {
        "id": "At5bb0GTrVWH"
      },
      "id": "At5bb0GTrVWH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63e646c8",
      "metadata": {
        "id": "63e646c8"
      },
      "outputs": [],
      "source": [
        "# Define the context and the question, limit of 512 tokens (between context, question and output)\n",
        "context = r\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "question = \"\"\n",
        "\n",
        "# Get the answer\n",
        "answer =\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6c4e773",
      "metadata": {
        "id": "c6c4e773"
      },
      "source": [
        "## Table Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TapexTokenizer.from_pretrained(\"microsoft/tapex-base-finetuned-wikisql\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"microsoft/tapex-base-finetuned-wikisql\")"
      ],
      "metadata": {
        "id": "gpi9HRimrgUd"
      },
      "id": "gpi9HRimrgUd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49ab217e",
      "metadata": {
        "id": "49ab217e"
      },
      "outputs": [],
      "source": [
        "table = pd.read_csv('')\n",
        "\n",
        "table = table.applymap(lambda x: str(x) if not isinstance(x, str) else x) # add comments\n",
        "# pull up heading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55da50ce",
      "metadata": {
        "id": "55da50ce"
      },
      "outputs": [],
      "source": [
        "query = \"\"\n",
        "encoding = tokenizer(table=, query=, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "\n",
        "\n",
        "outputs =\n",
        "\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check your answer"
      ],
      "metadata": {
        "id": "I2bKkenhpziQ"
      },
      "id": "I2bKkenhpziQ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}