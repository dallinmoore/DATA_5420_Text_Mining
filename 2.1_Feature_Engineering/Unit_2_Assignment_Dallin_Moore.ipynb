{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Unit 2 Assignment**: Feature Engineering \\& Supervised Classification\n",
        "## *DATA 5420/6420*\n",
        "## **Name:** Dallin Moore\n",
        "\n",
        "In this second assignment you will be tasked with training your own supervised classification model, this could be to do document classification of some sort, or a sentiment analysis. You will first be tasked with selecting a labeled text dataset to train a supervised classifier, then you will apply it to your dataset from Unit 1.\n",
        "\n",
        "Next, you will find a pretrained supervised model from Hugging Face, which has a larger collection of pretrained document classification and sentiment analysis models. You will investigate the results of the model you trained against the pretrained model and compare their performances. This will help you decide how you might incorporate some form of either document classification or sentiment analysis into your final product.\n",
        "\n",
        "**General breakdown of steps**:\n",
        "\n",
        "\n",
        "1.   Select a labeled dataset to perform document classification or sentiment analysis\n",
        "2.   Train at least two different models on the dataset, compare performance\n",
        "3.   Apply the classification model to your dataset from Unit 1\n",
        "4.   Examine results, speak to how well it appears to perform\n",
        "5.   Apply a pretrained transformer model to your dataset from Unit 1\n",
        "6.   Examine results, speak to how well it appears to perform\n",
        "7.   Compare and contrast your trained model vs the pretrained model\n",
        "\n",
        "**Some suggested datasets for document classification**:\n",
        "\n",
        "\n",
        "*   Brown Corpus -- accesible through NLTK\n",
        "*   20 News Groups -- accessible through scikit learn\n",
        "*   [Yelp Reviews Dataset](https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset)\n",
        "\n",
        "**Some suggetsed datasets for sentiment analysis**:\n",
        "\n",
        "*   [IMDB movie reviews](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)\n",
        "*   [Sentiment140](https://www.kaggle.com/datasets/kazanova/sentiment140)\n",
        "*   Yelp Reviews Dataset - linked above\n",
        "\n",
        "You are by no means limited to these datasets, [Kaggle](https://www.kaggle.com/datasets) has lots of datasets available for document classification and sentiment analysis, so you may find something more relevant to your dataset there. Just make sure it it labeled data (i.e., has a labeled class like positive, negative).\n",
        "\n",
        "\n",
        "**Pretrained Models**:\n",
        "\n",
        "You can find pretrained models for sentiment analysis and document classification on the models page for [HuggingFace](https://huggingface.co/models?pipeline_tag=text-classification&sort=trending). Remember, tools like Poe, ChatGPT, Claude, etc. are excellent resources for developing code for implementing models such as these!!\n",
        "\n",
        "Try something like: *I need a pretrained model from hugging face to do XYZ, can you provide python code*"
      ],
      "metadata": {
        "id": "BQ7_zrkOZuXv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nn6gax7jZq-5"
      },
      "outputs": [],
      "source": [
        "# import dependencies\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import re\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load in selected labeled dataset\n",
        "df_songs = pd.read_csv('/content/Top-1000-Songs-To-Hear-Before-You-Die.csv')\n",
        "# load in the lyrics dataset (to add lyrics to the genre)\n",
        "df_lyrics = pd.read_csv('/content/preprocessed-spotify-dataset.csv')\n",
        "df_songs.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "b_jTWrrknrec",
        "outputId": "b1f65f0f-7551-48b1-f7fe-4687c02f15ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Artist        Theme                                              Title  \\\n",
              "0         !!!      Protest  Me and Giuliani Down By the Schoolyard (A True...   \n",
              "1   808 State  Party songs                                      Pacific State   \n",
              "2  A R Rahman         Love                                       Kehna hi kya   \n",
              "3     Aaliyah          Sex                                          Try Again   \n",
              "4        Abba   Heartbreak                            The Winner Takes It All   \n",
              "\n",
              "   Year (Top 1000 Songs To Hear Before You Die.csv)  \n",
              "0                                              2003  \n",
              "1                                              1989  \n",
              "2                                              1995  \n",
              "3                                              2000  \n",
              "4                                              1980  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-702e14f6-ea7f-4159-ab63-d715088b55e5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Artist</th>\n",
              "      <th>Theme</th>\n",
              "      <th>Title</th>\n",
              "      <th>Year (Top 1000 Songs To Hear Before You Die.csv)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>!!!</td>\n",
              "      <td>Protest</td>\n",
              "      <td>Me and Giuliani Down By the Schoolyard (A True...</td>\n",
              "      <td>2003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>808 State</td>\n",
              "      <td>Party songs</td>\n",
              "      <td>Pacific State</td>\n",
              "      <td>1989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A R Rahman</td>\n",
              "      <td>Love</td>\n",
              "      <td>Kehna hi kya</td>\n",
              "      <td>1995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aaliyah</td>\n",
              "      <td>Sex</td>\n",
              "      <td>Try Again</td>\n",
              "      <td>2000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Abba</td>\n",
              "      <td>Heartbreak</td>\n",
              "      <td>The Winner Takes It All</td>\n",
              "      <td>1980</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-702e14f6-ea7f-4159-ab63-d715088b55e5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-702e14f6-ea7f-4159-ab63-d715088b55e5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-702e14f6-ea7f-4159-ab63-d715088b55e5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1225383b-9c9a-4322-85a1-73ab683d07c1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1225383b-9c9a-4322-85a1-73ab683d07c1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1225383b-9c9a-4322-85a1-73ab683d07c1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_songs",
              "summary": "{\n  \"name\": \"df_songs\",\n  \"rows\": 993,\n  \"fields\": [\n    {\n      \"column\": \"Artist\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 667,\n        \"samples\": [\n          \"Whitney Houston\",\n          \"Dusty Springfield\",\n          \"Todd Rundgren\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Theme\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Protest\",\n          \"Party songs\",\n          \"Life and death\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 981,\n        \"samples\": [\n          \"Drive\",\n          \"I Heard it Through the Grapevine\",\n          \"Cruisin\\u2019 the Streets\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Year (Top 1000 Songs To Hear Before You Die.csv)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 15,\n        \"min\": 1916,\n        \"max\": 2008,\n        \"num_unique_values\": 74,\n        \"samples\": [\n          1980,\n          1950,\n          1967\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Will you be performing document classification or sentiment analysis? What is your outcome variable (i.e., positive, negative, genre type, etc.)**"
      ],
      "metadata": {
        "id": "N7484K9YvqNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Document classification predicting the genre of spefic songs. Because the dataset selected doesn't have lyrics, I will create a new DataFrame with the name, artists, lyrics, and list of genres"
      ],
      "metadata": {
        "id": "dZsSfH5rv25q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Which dataset did you decide to go with and why?**"
      ],
      "metadata": {
        "id": "hTR4UCSdvlne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I went with a dataset containing the top 1,000 songs to listen to before you die (the guardian). It includes the them that I want for classifying my lyrics."
      ],
      "metadata": {
        "id": "1u2PnZ9Xv_vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess strings\n",
        "def preprocess_string(s):\n",
        "    # Remove non-alphanumeric characters\n",
        "    s = re.sub(r'[^a-zA-Z0-9\\s]', '', s)\n",
        "    # Lowercase the string\n",
        "    s = s.lower()\n",
        "    return s.strip()\n",
        "\n",
        "# Function to match song and artist names\n",
        "def match_song_and_artist(song_name, artist_name, df_lyrics):\n",
        "    song_name_normalized = preprocess_string(song_name)\n",
        "    artist_name_normalized = preprocess_string(artist_name)\n",
        "    match = df_lyrics[(df_lyrics['song'].apply(preprocess_string) == song_name_normalized) &\n",
        "                      (df_lyrics['artist'].apply(preprocess_string) == artist_name_normalized)]\n",
        "    if not match.empty:\n",
        "        return match.iloc[0]['text']\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Add matched lyrics column to df_songs\n",
        "df_songs['Matched_Lyrics'] = df_songs.apply(lambda row: match_song_and_artist(row['Title'], row['Artist'], df_lyrics), axis=1)\n",
        "\n",
        "# Filter records to only contain those with both genre and lyrics\n",
        "df = df_songs.dropna(subset=['Theme', 'Matched_Lyrics'])\n",
        "\n",
        "# Select only the required columns\n",
        "df = df[['Title', 'Artist', 'Matched_Lyrics', 'Theme']]\n",
        "\n",
        "# Rename columns to match the specified names\n",
        "df.columns = ['Song Name', 'Artist', 'Lyrics', 'Theme']\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "GFblsjeSwA3I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "b369e971-16f7-4025-db50-76d9f4aeac2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      Song Name           Artist  \\\n",
              "4       The Winner Takes It All             Abba   \n",
              "5                 Dancing Queen             Abba   \n",
              "12                     Dream On        Aerosmith   \n",
              "13          Love in an Elevator        Aerosmith   \n",
              "28      Music to Watch Girls By    Andy Williams   \n",
              "..                          ...              ...   \n",
              "975                Coney Island     Van Morrison   \n",
              "977               Fujiyama Mama    Wanda Jackson   \n",
              "983             How Will I Know  Whitney Houston   \n",
              "984  Saving All My Love for You  Whitney Houston   \n",
              "988               Mustang Sally   Wilson Pickett   \n",
              "\n",
              "                                                Lyrics              Theme  \n",
              "4    I want talk thing go though hurt I history I p...         Heartbreak  \n",
              "5    dance jive time life see girl watch scene digg...        Party songs  \n",
              "12   every time I look mirror line face get clear p...     Life and death  \n",
              "13   workin like dog fo de boss man workin de compa...                Sex  \n",
              "28   boy watch girl girl watch boy watch girl go ey...               Love  \n",
              "..                                                 ...                ...  \n",
              "975  coney island come downpatrick stop st john poi...  People and places  \n",
              "977  I nagasaki hiroshima I baby I cause I fujiyama...                Sex  \n",
              "983  boy I know one I dream look eye take I cloud m...               Love  \n",
              "984  steal moment share get family need though I tr...                Sex  \n",
              "988  mustang sally think well slow mustang mustang ...  People and places  \n",
              "\n",
              "[177 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8b902bca-bf50-4748-b93f-d65bbd2b9889\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Song Name</th>\n",
              "      <th>Artist</th>\n",
              "      <th>Lyrics</th>\n",
              "      <th>Theme</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The Winner Takes It All</td>\n",
              "      <td>Abba</td>\n",
              "      <td>I want talk thing go though hurt I history I p...</td>\n",
              "      <td>Heartbreak</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Dancing Queen</td>\n",
              "      <td>Abba</td>\n",
              "      <td>dance jive time life see girl watch scene digg...</td>\n",
              "      <td>Party songs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Dream On</td>\n",
              "      <td>Aerosmith</td>\n",
              "      <td>every time I look mirror line face get clear p...</td>\n",
              "      <td>Life and death</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Love in an Elevator</td>\n",
              "      <td>Aerosmith</td>\n",
              "      <td>workin like dog fo de boss man workin de compa...</td>\n",
              "      <td>Sex</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Music to Watch Girls By</td>\n",
              "      <td>Andy Williams</td>\n",
              "      <td>boy watch girl girl watch boy watch girl go ey...</td>\n",
              "      <td>Love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>975</th>\n",
              "      <td>Coney Island</td>\n",
              "      <td>Van Morrison</td>\n",
              "      <td>coney island come downpatrick stop st john poi...</td>\n",
              "      <td>People and places</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>977</th>\n",
              "      <td>Fujiyama Mama</td>\n",
              "      <td>Wanda Jackson</td>\n",
              "      <td>I nagasaki hiroshima I baby I cause I fujiyama...</td>\n",
              "      <td>Sex</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>983</th>\n",
              "      <td>How Will I Know</td>\n",
              "      <td>Whitney Houston</td>\n",
              "      <td>boy I know one I dream look eye take I cloud m...</td>\n",
              "      <td>Love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>984</th>\n",
              "      <td>Saving All My Love for You</td>\n",
              "      <td>Whitney Houston</td>\n",
              "      <td>steal moment share get family need though I tr...</td>\n",
              "      <td>Sex</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>988</th>\n",
              "      <td>Mustang Sally</td>\n",
              "      <td>Wilson Pickett</td>\n",
              "      <td>mustang sally think well slow mustang mustang ...</td>\n",
              "      <td>People and places</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>177 rows Ã— 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8b902bca-bf50-4748-b93f-d65bbd2b9889')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8b902bca-bf50-4748-b93f-d65bbd2b9889 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8b902bca-bf50-4748-b93f-d65bbd2b9889');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f9407f51-6bfa-45d6-8188-0ccde0316a26\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f9407f51-6bfa-45d6-8188-0ccde0316a26')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f9407f51-6bfa-45d6-8188-0ccde0316a26 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 177,\n  \"fields\": [\n    {\n      \"column\": \"Song Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 176,\n        \"samples\": [\n          \"Death Is Not the End\",\n          \"Vincent\",\n          \"Crying\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Artist\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 105,\n        \"samples\": [\n          \"Emmylou Harris\",\n          \"Nick Cave\",\n          \"New Order\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Lyrics\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 176,\n        \"samples\": [\n          \"sad lonely get friend remember death end hell sacred fall dow bend remember death end end end remember death end stand crossroad comprehend remember death end dream vanish know bend remember death end end end remember death end storm cloud gather round heavy rain descend remember death end nowhere comfort help hand lend remember death end end end remember death end oh tree life grow spirit never die bright light salvation shine dark empty sky city fire burn flesh man remember death end search vain find one law abide citizen remember death end end end remember death end\",\n          \"starry starry night paint palette blue gray look summer day eye know darkness soul shadow hill sketch tree daffodil catch breeze winter chill color snowy linen land I understand try say I suffer sanity try set free would listen know perhaps listen starry starry night flame flower brightly blaze swirl cloud violet haze reflect vincent eye china blue color change hue morning field amber grain weather face line pain soothe beneath artist love hand I understand try say I suffer sanity try set free would listen know perhaps listen could love still love true hope leave sight starry starry night take life lover often I could tell vincent world never mean one beautiful starry starry night portrait hang empty hall frameless head nameless wall eye watch world forget like stranger meet ragged man ragged clothe silver thorn bloody rose lie crush break virgin snow I think I know try say I suffer sanity try set free would listen listen still perhaps never\",\n          \"I right I could smile I see last night hold hand tight stop say hello aw wish I well could tell I cry cry say long leave I stand alone alone cry cry cry cry hard understand touch hand start I cry I think I true true I love even I darle I love I I always cry cry yes go moment I cry cry cry cry yeah cry cry\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Theme\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Heartbreak\",\n          \"Party songs\",\n          \"People and places\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " df['Theme'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkNnrpBAZNpL",
        "outputId": "c3114cae-3d7a-4255-c83a-6a18b792d1b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Heartbreak           34\n",
              "People and places    33\n",
              "Life and death       26\n",
              "Love                 25\n",
              "Sex                  23\n",
              "Party songs          18\n",
              "Protest              18\n",
              "Name: Theme, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What, if any cleaning or text normalization steps did you apply to this dataset and why?**"
      ],
      "metadata": {
        "id": "j40NCFwpwFct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The lyrics data was already cleaned, however the artist and song title needed to be lowered and punctuation removed to be match up the lyrics with the theme."
      ],
      "metadata": {
        "id": "kBmKaU1UwKAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# perform feature engineering on your cleaned corpus\n",
        "# Split data into text and labels\n",
        "texts = df['Lyrics'].tolist()\n",
        "labels = df['Theme'].tolist()\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=.2, random_state=42)\n",
        "# Create a TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words = 'english', min_df=1, max_df=0.8, ngram_range=(1, 2))\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "num_features_train = X_train_vec.shape[1]\n",
        "num_features_test = X_test_vec.shape[1]"
      ],
      "metadata": {
        "id": "Uwj0TUHbxfq_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08c47912-7ba8-4958-eee7-4be0ceda4c9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10546 10546\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Which form of feature engineering did you choose (count or TFIDF) and did you go with unigrams, bigrams, etc.? Why?**"
      ],
      "metadata": {
        "id": "pxED96uWxkRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**min_df**: Since genre classification might involve specialized language or jargon that appears only in a few documents, setting min_df too high might cause you to miss out on important features, so it is set to 1 to start.\n",
        "\n",
        "**max_df**: For genre classification, we want to exclude words that appear too frequently across all genres, as they are less likely to be discriminative. A value of max_df=0.8 will be used to start, but I will consider raising it to max_df=0.95 to improve the model.\n",
        "\n",
        "**ngram_range**: For lyrics, considering single words (unigrams) and pairs of words (bigrams) might capture more meaningful phrases and patterns in the text. I will start with a range that includes unigrams and bigrams (ngram_range=(1, 2)), but I might experiment with higher values depending on the results."
      ],
      "metadata": {
        "id": "07FuHQb6xqvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next, train your supervised classifier. Remember:**\n",
        "\n",
        "\n",
        "\n",
        "*   Create at least a training and a test set (fine if you don't have enough data to do a validation set)\n",
        "*   Perform cross-validation\n",
        "*   Train at least two different supervised classifiers on your training set\n",
        "*   If in the 6420 section, also plan to try out at least two changes to the model parameters\n",
        "* Apply your best performing model to the test set\n",
        "* Provide model evaluation metrics\n",
        "\n"
      ],
      "metadata": {
        "id": "Q_EzBUnywYti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    'Naive Bayes': MultinomialNB(),\n",
        "    'Linear SVM': LinearSVC(random_state=42),\n",
        "    'Random Forests': RandomForestClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "# Perform cross-validation for each model\n",
        "for name, model in models.items():\n",
        "    scores = cross_val_score(model,X_train_vec,y_train,cv=5)\n",
        "    print(f\"{name} Cross-Validation Mean Accuracy: {scores.mean():.4f}\")"
      ],
      "metadata": {
        "id": "39suNeoPw_Ex",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acaeca2d-c591-4907-b3c4-d7409e6ccaa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Cross-Validation Mean Accuracy: 0.2266\n",
            "Linear SVM Cross-Validation Mean Accuracy: 0.3256\n",
            "Random Forests Cross-Validation Mean Accuracy: 0.2690\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Which model performed best and how do you know?**"
      ],
      "metadata": {
        "id": "P3nft34Lw_fn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Linear SVM performed the best. Unfortunately, with the small dataset it was only 33% accurate."
      ],
      "metadata": {
        "id": "gmpmkGDbxIKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now, bring in your dataset from Unit 1 and apply your best performing model to add labels to this dataset (sentiment or document class). Remember:**\n",
        "\n",
        "*   Apply the same cleaning and text normalization steps to this dataset as you did the training data\n",
        "*   Apply the same feature engineering type and parameters\n",
        "*   Use the `.transform()` on your Unit 1 dataset with the vectorizer to ensure you match the number of features used to train your model\n",
        "*  Store the predictions and your text observations in a dataframe\n",
        "\n"
      ],
      "metadata": {
        "id": "7fJZUP7ixJJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_svc = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10],\n",
        "    'loss': ['hinge', 'squared_hinge'],\n",
        "    'max_iter': [1000, 5000, 10000]\n",
        "}\n",
        "\n",
        "# Grid search for LinearSVC\n",
        "grid_search_svc = GridSearchCV(LinearSVC(random_state=42), param_grid_svc, cv=5, verbose=10)\n",
        "grid_search_svc.fit(X_train_vec, y_train)\n",
        "\n",
        "# After fitting, you would typically print the best parameters as follows:\n",
        "print(\"Best parameters for LinearSVC:\", grid_search_svc.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAYyR45WpoGG",
        "outputId": "dab18e5a-a380-43a4-c727-f01a31c3cc90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
            "[CV 1/5; 1/30] START C=0.001, loss=hinge, max_iter=1000.........................\n",
            "[CV 1/5; 1/30] END C=0.001, loss=hinge, max_iter=1000;, score=0.241 total time=   0.0s\n",
            "[CV 2/5; 1/30] START C=0.001, loss=hinge, max_iter=1000.........................\n",
            "[CV 2/5; 1/30] END C=0.001, loss=hinge, max_iter=1000;, score=0.214 total time=   0.0s\n",
            "[CV 3/5; 1/30] START C=0.001, loss=hinge, max_iter=1000.........................\n",
            "[CV 3/5; 1/30] END C=0.001, loss=hinge, max_iter=1000;, score=0.214 total time=   0.0s\n",
            "[CV 4/5; 1/30] START C=0.001, loss=hinge, max_iter=1000.........................\n",
            "[CV 4/5; 1/30] END C=0.001, loss=hinge, max_iter=1000;, score=0.214 total time=   0.0s\n",
            "[CV 5/5; 1/30] START C=0.001, loss=hinge, max_iter=1000.........................\n",
            "[CV 5/5; 1/30] END C=0.001, loss=hinge, max_iter=1000;, score=0.250 total time=   0.0s\n",
            "[CV 1/5; 2/30] START C=0.001, loss=hinge, max_iter=5000.........................\n",
            "[CV 1/5; 2/30] END C=0.001, loss=hinge, max_iter=5000;, score=0.241 total time=   0.0s\n",
            "[CV 2/5; 2/30] START C=0.001, loss=hinge, max_iter=5000.........................\n",
            "[CV 2/5; 2/30] END C=0.001, loss=hinge, max_iter=5000;, score=0.214 total time=   0.0s\n",
            "[CV 3/5; 2/30] START C=0.001, loss=hinge, max_iter=5000.........................\n",
            "[CV 3/5; 2/30] END C=0.001, loss=hinge, max_iter=5000;, score=0.214 total time=   0.0s\n",
            "[CV 4/5; 2/30] START C=0.001, loss=hinge, max_iter=5000.........................\n",
            "[CV 4/5; 2/30] END C=0.001, loss=hinge, max_iter=5000;, score=0.214 total time=   0.0s\n",
            "[CV 5/5; 2/30] START C=0.001, loss=hinge, max_iter=5000.........................\n",
            "[CV 5/5; 2/30] END C=0.001, loss=hinge, max_iter=5000;, score=0.250 total time=   0.0s\n",
            "[CV 1/5; 3/30] START C=0.001, loss=hinge, max_iter=10000........................\n",
            "[CV 1/5; 3/30] END C=0.001, loss=hinge, max_iter=10000;, score=0.241 total time=   0.0s\n",
            "[CV 2/5; 3/30] START C=0.001, loss=hinge, max_iter=10000........................\n",
            "[CV 2/5; 3/30] END C=0.001, loss=hinge, max_iter=10000;, score=0.214 total time=   0.0s\n",
            "[CV 3/5; 3/30] START C=0.001, loss=hinge, max_iter=10000........................\n",
            "[CV 3/5; 3/30] END C=0.001, loss=hinge, max_iter=10000;, score=0.214 total time=   0.0s\n",
            "[CV 4/5; 3/30] START C=0.001, loss=hinge, max_iter=10000........................\n",
            "[CV 4/5; 3/30] END C=0.001, loss=hinge, max_iter=10000;, score=0.214 total time=   0.0s\n",
            "[CV 5/5; 3/30] START C=0.001, loss=hinge, max_iter=10000........................\n",
            "[CV 5/5; 3/30] END C=0.001, loss=hinge, max_iter=10000;, score=0.250 total time=   0.0s\n",
            "[CV 1/5; 4/30] START C=0.001, loss=squared_hinge, max_iter=1000.................\n",
            "[CV 1/5; 4/30] END C=0.001, loss=squared_hinge, max_iter=1000;, score=0.241 total time=   0.0s\n",
            "[CV 2/5; 4/30] START C=0.001, loss=squared_hinge, max_iter=1000.................\n",
            "[CV 2/5; 4/30] END C=0.001, loss=squared_hinge, max_iter=1000;, score=0.214 total time=   0.0s\n",
            "[CV 3/5; 4/30] START C=0.001, loss=squared_hinge, max_iter=1000.................\n",
            "[CV 3/5; 4/30] END C=0.001, loss=squared_hinge, max_iter=1000;, score=0.214 total time=   0.0s\n",
            "[CV 4/5; 4/30] START C=0.001, loss=squared_hinge, max_iter=1000.................\n",
            "[CV 4/5; 4/30] END C=0.001, loss=squared_hinge, max_iter=1000;, score=0.214 total time=   0.0s\n",
            "[CV 5/5; 4/30] START C=0.001, loss=squared_hinge, max_iter=1000.................\n",
            "[CV 5/5; 4/30] END C=0.001, loss=squared_hinge, max_iter=1000;, score=0.250 total time=   0.0s\n",
            "[CV 1/5; 5/30] START C=0.001, loss=squared_hinge, max_iter=5000.................\n",
            "[CV 1/5; 5/30] END C=0.001, loss=squared_hinge, max_iter=5000;, score=0.241 total time=   0.0s\n",
            "[CV 2/5; 5/30] START C=0.001, loss=squared_hinge, max_iter=5000.................\n",
            "[CV 2/5; 5/30] END C=0.001, loss=squared_hinge, max_iter=5000;, score=0.214 total time=   0.0s\n",
            "[CV 3/5; 5/30] START C=0.001, loss=squared_hinge, max_iter=5000.................\n",
            "[CV 3/5; 5/30] END C=0.001, loss=squared_hinge, max_iter=5000;, score=0.214 total time=   0.0s\n",
            "[CV 4/5; 5/30] START C=0.001, loss=squared_hinge, max_iter=5000.................\n",
            "[CV 4/5; 5/30] END C=0.001, loss=squared_hinge, max_iter=5000;, score=0.214 total time=   0.0s\n",
            "[CV 5/5; 5/30] START C=0.001, loss=squared_hinge, max_iter=5000.................\n",
            "[CV 5/5; 5/30] END C=0.001, loss=squared_hinge, max_iter=5000;, score=0.250 total time=   0.0s\n",
            "[CV 1/5; 6/30] START C=0.001, loss=squared_hinge, max_iter=10000................\n",
            "[CV 1/5; 6/30] END C=0.001, loss=squared_hinge, max_iter=10000;, score=0.241 total time=   0.0s\n",
            "[CV 2/5; 6/30] START C=0.001, loss=squared_hinge, max_iter=10000................\n",
            "[CV 2/5; 6/30] END C=0.001, loss=squared_hinge, max_iter=10000;, score=0.214 total time=   0.0s\n",
            "[CV 3/5; 6/30] START C=0.001, loss=squared_hinge, max_iter=10000................\n",
            "[CV 3/5; 6/30] END C=0.001, loss=squared_hinge, max_iter=10000;, score=0.214 total time=   0.0s\n",
            "[CV 4/5; 6/30] START C=0.001, loss=squared_hinge, max_iter=10000................\n",
            "[CV 4/5; 6/30] END C=0.001, loss=squared_hinge, max_iter=10000;, score=0.214 total time=   0.0s\n",
            "[CV 5/5; 6/30] START C=0.001, loss=squared_hinge, max_iter=10000................\n",
            "[CV 5/5; 6/30] END C=0.001, loss=squared_hinge, max_iter=10000;, score=0.250 total time=   0.0s\n",
            "[CV 1/5; 7/30] START C=0.01, loss=hinge, max_iter=1000..........................\n",
            "[CV 1/5; 7/30] END C=0.01, loss=hinge, max_iter=1000;, score=0.241 total time=   0.0s\n",
            "[CV 2/5; 7/30] START C=0.01, loss=hinge, max_iter=1000..........................\n",
            "[CV 2/5; 7/30] END C=0.01, loss=hinge, max_iter=1000;, score=0.214 total time=   0.0s\n",
            "[CV 3/5; 7/30] START C=0.01, loss=hinge, max_iter=1000..........................\n",
            "[CV 3/5; 7/30] END C=0.01, loss=hinge, max_iter=1000;, score=0.214 total time=   0.0s\n",
            "[CV 4/5; 7/30] START C=0.01, loss=hinge, max_iter=1000..........................\n",
            "[CV 4/5; 7/30] END C=0.01, loss=hinge, max_iter=1000;, score=0.214 total time=   0.0s\n",
            "[CV 5/5; 7/30] START C=0.01, loss=hinge, max_iter=1000..........................\n",
            "[CV 5/5; 7/30] END C=0.01, loss=hinge, max_iter=1000;, score=0.250 total time=   0.0s\n",
            "[CV 1/5; 8/30] START C=0.01, loss=hinge, max_iter=5000..........................\n",
            "[CV 1/5; 8/30] END C=0.01, loss=hinge, max_iter=5000;, score=0.241 total time=   0.0s\n",
            "[CV 2/5; 8/30] START C=0.01, loss=hinge, max_iter=5000..........................\n",
            "[CV 2/5; 8/30] END C=0.01, loss=hinge, max_iter=5000;, score=0.214 total time=   0.0s\n",
            "[CV 3/5; 8/30] START C=0.01, loss=hinge, max_iter=5000..........................\n",
            "[CV 3/5; 8/30] END C=0.01, loss=hinge, max_iter=5000;, score=0.214 total time=   0.0s\n",
            "[CV 4/5; 8/30] START C=0.01, loss=hinge, max_iter=5000..........................\n",
            "[CV 4/5; 8/30] END C=0.01, loss=hinge, max_iter=5000;, score=0.214 total time=   0.0s\n",
            "[CV 5/5; 8/30] START C=0.01, loss=hinge, max_iter=5000..........................\n",
            "[CV 5/5; 8/30] END C=0.01, loss=hinge, max_iter=5000;, score=0.250 total time=   0.0s\n",
            "[CV 1/5; 9/30] START C=0.01, loss=hinge, max_iter=10000.........................\n",
            "[CV 1/5; 9/30] END C=0.01, loss=hinge, max_iter=10000;, score=0.241 total time=   0.0s\n",
            "[CV 2/5; 9/30] START C=0.01, loss=hinge, max_iter=10000.........................\n",
            "[CV 2/5; 9/30] END C=0.01, loss=hinge, max_iter=10000;, score=0.214 total time=   0.0s\n",
            "[CV 3/5; 9/30] START C=0.01, loss=hinge, max_iter=10000.........................\n",
            "[CV 3/5; 9/30] END C=0.01, loss=hinge, max_iter=10000;, score=0.214 total time=   0.0s\n",
            "[CV 4/5; 9/30] START C=0.01, loss=hinge, max_iter=10000.........................\n",
            "[CV 4/5; 9/30] END C=0.01, loss=hinge, max_iter=10000;, score=0.214 total time=   0.0s\n",
            "[CV 5/5; 9/30] START C=0.01, loss=hinge, max_iter=10000.........................\n",
            "[CV 5/5; 9/30] END C=0.01, loss=hinge, max_iter=10000;, score=0.250 total time=   0.0s\n",
            "[CV 1/5; 10/30] START C=0.01, loss=squared_hinge, max_iter=1000.................\n",
            "[CV 1/5; 10/30] END C=0.01, loss=squared_hinge, max_iter=1000;, score=0.241 total time=   0.0s\n",
            "[CV 2/5; 10/30] START C=0.01, loss=squared_hinge, max_iter=1000.................\n",
            "[CV 2/5; 10/30] END C=0.01, loss=squared_hinge, max_iter=1000;, score=0.214 total time=   0.0s\n",
            "[CV 3/5; 10/30] START C=0.01, loss=squared_hinge, max_iter=1000.................\n",
            "[CV 3/5; 10/30] END C=0.01, loss=squared_hinge, max_iter=1000;, score=0.214 total time=   0.0s\n",
            "[CV 4/5; 10/30] START C=0.01, loss=squared_hinge, max_iter=1000.................\n",
            "[CV 4/5; 10/30] END C=0.01, loss=squared_hinge, max_iter=1000;, score=0.214 total time=   0.0s\n",
            "[CV 5/5; 10/30] START C=0.01, loss=squared_hinge, max_iter=1000.................\n",
            "[CV 5/5; 10/30] END C=0.01, loss=squared_hinge, max_iter=1000;, score=0.250 total time=   0.0s\n",
            "[CV 1/5; 11/30] START C=0.01, loss=squared_hinge, max_iter=5000.................\n",
            "[CV 1/5; 11/30] END C=0.01, loss=squared_hinge, max_iter=5000;, score=0.241 total time=   0.0s\n",
            "[CV 2/5; 11/30] START C=0.01, loss=squared_hinge, max_iter=5000.................\n",
            "[CV 2/5; 11/30] END C=0.01, loss=squared_hinge, max_iter=5000;, score=0.214 total time=   0.0s\n",
            "[CV 3/5; 11/30] START C=0.01, loss=squared_hinge, max_iter=5000.................\n",
            "[CV 3/5; 11/30] END C=0.01, loss=squared_hinge, max_iter=5000;, score=0.214 total time=   0.0s\n",
            "[CV 4/5; 11/30] START C=0.01, loss=squared_hinge, max_iter=5000.................\n",
            "[CV 4/5; 11/30] END C=0.01, loss=squared_hinge, max_iter=5000;, score=0.214 total time=   0.0s\n",
            "[CV 5/5; 11/30] START C=0.01, loss=squared_hinge, max_iter=5000.................\n",
            "[CV 5/5; 11/30] END C=0.01, loss=squared_hinge, max_iter=5000;, score=0.250 total time=   0.0s\n",
            "[CV 1/5; 12/30] START C=0.01, loss=squared_hinge, max_iter=10000................\n",
            "[CV 1/5; 12/30] END C=0.01, loss=squared_hinge, max_iter=10000;, score=0.241 total time=   0.0s\n",
            "[CV 2/5; 12/30] START C=0.01, loss=squared_hinge, max_iter=10000................\n",
            "[CV 2/5; 12/30] END C=0.01, loss=squared_hinge, max_iter=10000;, score=0.214 total time=   0.0s\n",
            "[CV 3/5; 12/30] START C=0.01, loss=squared_hinge, max_iter=10000................\n",
            "[CV 3/5; 12/30] END C=0.01, loss=squared_hinge, max_iter=10000;, score=0.214 total time=   0.0s\n",
            "[CV 4/5; 12/30] START C=0.01, loss=squared_hinge, max_iter=10000................\n",
            "[CV 4/5; 12/30] END C=0.01, loss=squared_hinge, max_iter=10000;, score=0.214 total time=   0.0s\n",
            "[CV 5/5; 12/30] START C=0.01, loss=squared_hinge, max_iter=10000................\n",
            "[CV 5/5; 12/30] END C=0.01, loss=squared_hinge, max_iter=10000;, score=0.250 total time=   0.0s\n",
            "[CV 1/5; 13/30] START C=0.1, loss=hinge, max_iter=1000..........................\n",
            "[CV 1/5; 13/30] END C=0.1, loss=hinge, max_iter=1000;, score=0.379 total time=   0.0s\n",
            "[CV 2/5; 13/30] START C=0.1, loss=hinge, max_iter=1000..........................\n",
            "[CV 2/5; 13/30] END C=0.1, loss=hinge, max_iter=1000;, score=0.179 total time=   0.0s\n",
            "[CV 3/5; 13/30] START C=0.1, loss=hinge, max_iter=1000..........................\n",
            "[CV 3/5; 13/30] END C=0.1, loss=hinge, max_iter=1000;, score=0.357 total time=   0.0s\n",
            "[CV 4/5; 13/30] START C=0.1, loss=hinge, max_iter=1000..........................\n",
            "[CV 4/5; 13/30] END C=0.1, loss=hinge, max_iter=1000;, score=0.321 total time=   0.0s\n",
            "[CV 5/5; 13/30] START C=0.1, loss=hinge, max_iter=1000..........................\n",
            "[CV 5/5; 13/30] END C=0.1, loss=hinge, max_iter=1000;, score=0.393 total time=   0.0s\n",
            "[CV 1/5; 14/30] START C=0.1, loss=hinge, max_iter=5000..........................\n",
            "[CV 1/5; 14/30] END C=0.1, loss=hinge, max_iter=5000;, score=0.379 total time=   0.0s\n",
            "[CV 2/5; 14/30] START C=0.1, loss=hinge, max_iter=5000..........................\n",
            "[CV 2/5; 14/30] END C=0.1, loss=hinge, max_iter=5000;, score=0.179 total time=   0.0s\n",
            "[CV 3/5; 14/30] START C=0.1, loss=hinge, max_iter=5000..........................\n",
            "[CV 3/5; 14/30] END C=0.1, loss=hinge, max_iter=5000;, score=0.357 total time=   0.0s\n",
            "[CV 4/5; 14/30] START C=0.1, loss=hinge, max_iter=5000..........................\n",
            "[CV 4/5; 14/30] END C=0.1, loss=hinge, max_iter=5000;, score=0.321 total time=   0.0s\n",
            "[CV 5/5; 14/30] START C=0.1, loss=hinge, max_iter=5000..........................\n",
            "[CV 5/5; 14/30] END C=0.1, loss=hinge, max_iter=5000;, score=0.393 total time=   0.0s\n",
            "[CV 1/5; 15/30] START C=0.1, loss=hinge, max_iter=10000.........................\n",
            "[CV 1/5; 15/30] END C=0.1, loss=hinge, max_iter=10000;, score=0.379 total time=   0.0s\n",
            "[CV 2/5; 15/30] START C=0.1, loss=hinge, max_iter=10000.........................\n",
            "[CV 2/5; 15/30] END C=0.1, loss=hinge, max_iter=10000;, score=0.179 total time=   0.0s\n",
            "[CV 3/5; 15/30] START C=0.1, loss=hinge, max_iter=10000.........................\n",
            "[CV 3/5; 15/30] END C=0.1, loss=hinge, max_iter=10000;, score=0.357 total time=   0.0s\n",
            "[CV 4/5; 15/30] START C=0.1, loss=hinge, max_iter=10000.........................\n",
            "[CV 4/5; 15/30] END C=0.1, loss=hinge, max_iter=10000;, score=0.321 total time=   0.0s\n",
            "[CV 5/5; 15/30] START C=0.1, loss=hinge, max_iter=10000.........................\n",
            "[CV 5/5; 15/30] END C=0.1, loss=hinge, max_iter=10000;, score=0.393 total time=   0.0s\n",
            "[CV 1/5; 16/30] START C=0.1, loss=squared_hinge, max_iter=1000..................\n",
            "[CV 1/5; 16/30] END C=0.1, loss=squared_hinge, max_iter=1000;, score=0.276 total time=   0.0s\n",
            "[CV 2/5; 16/30] START C=0.1, loss=squared_hinge, max_iter=1000..................\n",
            "[CV 2/5; 16/30] END C=0.1, loss=squared_hinge, max_iter=1000;, score=0.179 total time=   0.0s\n",
            "[CV 3/5; 16/30] START C=0.1, loss=squared_hinge, max_iter=1000..................\n",
            "[CV 3/5; 16/30] END C=0.1, loss=squared_hinge, max_iter=1000;, score=0.250 total time=   0.0s\n",
            "[CV 4/5; 16/30] START C=0.1, loss=squared_hinge, max_iter=1000..................\n",
            "[CV 4/5; 16/30] END C=0.1, loss=squared_hinge, max_iter=1000;, score=0.214 total time=   0.0s\n",
            "[CV 5/5; 16/30] START C=0.1, loss=squared_hinge, max_iter=1000..................\n",
            "[CV 5/5; 16/30] END C=0.1, loss=squared_hinge, max_iter=1000;, score=0.286 total time=   0.0s\n",
            "[CV 1/5; 17/30] START C=0.1, loss=squared_hinge, max_iter=5000..................\n",
            "[CV 1/5; 17/30] END C=0.1, loss=squared_hinge, max_iter=5000;, score=0.276 total time=   0.0s\n",
            "[CV 2/5; 17/30] START C=0.1, loss=squared_hinge, max_iter=5000..................\n",
            "[CV 2/5; 17/30] END C=0.1, loss=squared_hinge, max_iter=5000;, score=0.179 total time=   0.0s\n",
            "[CV 3/5; 17/30] START C=0.1, loss=squared_hinge, max_iter=5000..................\n",
            "[CV 3/5; 17/30] END C=0.1, loss=squared_hinge, max_iter=5000;, score=0.250 total time=   0.0s\n",
            "[CV 4/5; 17/30] START C=0.1, loss=squared_hinge, max_iter=5000..................\n",
            "[CV 4/5; 17/30] END C=0.1, loss=squared_hinge, max_iter=5000;, score=0.214 total time=   0.0s\n",
            "[CV 5/5; 17/30] START C=0.1, loss=squared_hinge, max_iter=5000..................\n",
            "[CV 5/5; 17/30] END C=0.1, loss=squared_hinge, max_iter=5000;, score=0.286 total time=   0.0s\n",
            "[CV 1/5; 18/30] START C=0.1, loss=squared_hinge, max_iter=10000.................\n",
            "[CV 1/5; 18/30] END C=0.1, loss=squared_hinge, max_iter=10000;, score=0.276 total time=   0.0s\n",
            "[CV 2/5; 18/30] START C=0.1, loss=squared_hinge, max_iter=10000.................\n",
            "[CV 2/5; 18/30] END C=0.1, loss=squared_hinge, max_iter=10000;, score=0.179 total time=   0.0s\n",
            "[CV 3/5; 18/30] START C=0.1, loss=squared_hinge, max_iter=10000.................\n",
            "[CV 3/5; 18/30] END C=0.1, loss=squared_hinge, max_iter=10000;, score=0.250 total time=   0.0s\n",
            "[CV 4/5; 18/30] START C=0.1, loss=squared_hinge, max_iter=10000.................\n",
            "[CV 4/5; 18/30] END C=0.1, loss=squared_hinge, max_iter=10000;, score=0.214 total time=   0.0s\n",
            "[CV 5/5; 18/30] START C=0.1, loss=squared_hinge, max_iter=10000.................\n",
            "[CV 5/5; 18/30] END C=0.1, loss=squared_hinge, max_iter=10000;, score=0.286 total time=   0.0s\n",
            "[CV 1/5; 19/30] START C=1, loss=hinge, max_iter=1000............................\n",
            "[CV 1/5; 19/30] END C=1, loss=hinge, max_iter=1000;, score=0.414 total time=   0.0s\n",
            "[CV 2/5; 19/30] START C=1, loss=hinge, max_iter=1000............................\n",
            "[CV 2/5; 19/30] END C=1, loss=hinge, max_iter=1000;, score=0.179 total time=   0.0s\n",
            "[CV 3/5; 19/30] START C=1, loss=hinge, max_iter=1000............................\n",
            "[CV 3/5; 19/30] END C=1, loss=hinge, max_iter=1000;, score=0.357 total time=   0.0s\n",
            "[CV 4/5; 19/30] START C=1, loss=hinge, max_iter=1000............................\n",
            "[CV 4/5; 19/30] END C=1, loss=hinge, max_iter=1000;, score=0.321 total time=   0.0s\n",
            "[CV 5/5; 19/30] START C=1, loss=hinge, max_iter=1000............................\n",
            "[CV 5/5; 19/30] END C=1, loss=hinge, max_iter=1000;, score=0.357 total time=   0.0s\n",
            "[CV 1/5; 20/30] START C=1, loss=hinge, max_iter=5000............................\n",
            "[CV 1/5; 20/30] END C=1, loss=hinge, max_iter=5000;, score=0.414 total time=   0.0s\n",
            "[CV 2/5; 20/30] START C=1, loss=hinge, max_iter=5000............................\n",
            "[CV 2/5; 20/30] END C=1, loss=hinge, max_iter=5000;, score=0.179 total time=   0.0s\n",
            "[CV 3/5; 20/30] START C=1, loss=hinge, max_iter=5000............................\n",
            "[CV 3/5; 20/30] END C=1, loss=hinge, max_iter=5000;, score=0.357 total time=   0.0s\n",
            "[CV 4/5; 20/30] START C=1, loss=hinge, max_iter=5000............................\n",
            "[CV 4/5; 20/30] END C=1, loss=hinge, max_iter=5000;, score=0.321 total time=   0.0s\n",
            "[CV 5/5; 20/30] START C=1, loss=hinge, max_iter=5000............................\n",
            "[CV 5/5; 20/30] END C=1, loss=hinge, max_iter=5000;, score=0.357 total time=   0.0s\n",
            "[CV 1/5; 21/30] START C=1, loss=hinge, max_iter=10000...........................\n",
            "[CV 1/5; 21/30] END C=1, loss=hinge, max_iter=10000;, score=0.414 total time=   0.0s\n",
            "[CV 2/5; 21/30] START C=1, loss=hinge, max_iter=10000...........................\n",
            "[CV 2/5; 21/30] END C=1, loss=hinge, max_iter=10000;, score=0.179 total time=   0.0s\n",
            "[CV 3/5; 21/30] START C=1, loss=hinge, max_iter=10000...........................\n",
            "[CV 3/5; 21/30] END C=1, loss=hinge, max_iter=10000;, score=0.357 total time=   0.0s\n",
            "[CV 4/5; 21/30] START C=1, loss=hinge, max_iter=10000...........................\n",
            "[CV 4/5; 21/30] END C=1, loss=hinge, max_iter=10000;, score=0.321 total time=   0.0s\n",
            "[CV 5/5; 21/30] START C=1, loss=hinge, max_iter=10000...........................\n",
            "[CV 5/5; 21/30] END C=1, loss=hinge, max_iter=10000;, score=0.357 total time=   0.0s\n",
            "[CV 1/5; 22/30] START C=1, loss=squared_hinge, max_iter=1000....................\n",
            "[CV 1/5; 22/30] END C=1, loss=squared_hinge, max_iter=1000;, score=0.414 total time=   0.0s\n",
            "[CV 2/5; 22/30] START C=1, loss=squared_hinge, max_iter=1000....................\n",
            "[CV 2/5; 22/30] END C=1, loss=squared_hinge, max_iter=1000;, score=0.179 total time=   0.0s\n",
            "[CV 3/5; 22/30] START C=1, loss=squared_hinge, max_iter=1000....................\n",
            "[CV 3/5; 22/30] END C=1, loss=squared_hinge, max_iter=1000;, score=0.357 total time=   0.0s\n",
            "[CV 4/5; 22/30] START C=1, loss=squared_hinge, max_iter=1000....................\n",
            "[CV 4/5; 22/30] END C=1, loss=squared_hinge, max_iter=1000;, score=0.321 total time=   0.0s\n",
            "[CV 5/5; 22/30] START C=1, loss=squared_hinge, max_iter=1000....................\n",
            "[CV 5/5; 22/30] END C=1, loss=squared_hinge, max_iter=1000;, score=0.357 total time=   0.0s\n",
            "[CV 1/5; 23/30] START C=1, loss=squared_hinge, max_iter=5000....................\n",
            "[CV 1/5; 23/30] END C=1, loss=squared_hinge, max_iter=5000;, score=0.414 total time=   0.0s\n",
            "[CV 2/5; 23/30] START C=1, loss=squared_hinge, max_iter=5000....................\n",
            "[CV 2/5; 23/30] END C=1, loss=squared_hinge, max_iter=5000;, score=0.179 total time=   0.0s\n",
            "[CV 3/5; 23/30] START C=1, loss=squared_hinge, max_iter=5000....................\n",
            "[CV 3/5; 23/30] END C=1, loss=squared_hinge, max_iter=5000;, score=0.357 total time=   0.0s\n",
            "[CV 4/5; 23/30] START C=1, loss=squared_hinge, max_iter=5000....................\n",
            "[CV 4/5; 23/30] END C=1, loss=squared_hinge, max_iter=5000;, score=0.321 total time=   0.0s\n",
            "[CV 5/5; 23/30] START C=1, loss=squared_hinge, max_iter=5000....................\n",
            "[CV 5/5; 23/30] END C=1, loss=squared_hinge, max_iter=5000;, score=0.357 total time=   0.0s\n",
            "[CV 1/5; 24/30] START C=1, loss=squared_hinge, max_iter=10000...................\n",
            "[CV 1/5; 24/30] END C=1, loss=squared_hinge, max_iter=10000;, score=0.414 total time=   0.0s\n",
            "[CV 2/5; 24/30] START C=1, loss=squared_hinge, max_iter=10000...................\n",
            "[CV 2/5; 24/30] END C=1, loss=squared_hinge, max_iter=10000;, score=0.179 total time=   0.0s\n",
            "[CV 3/5; 24/30] START C=1, loss=squared_hinge, max_iter=10000...................\n",
            "[CV 3/5; 24/30] END C=1, loss=squared_hinge, max_iter=10000;, score=0.357 total time=   0.0s\n",
            "[CV 4/5; 24/30] START C=1, loss=squared_hinge, max_iter=10000...................\n",
            "[CV 4/5; 24/30] END C=1, loss=squared_hinge, max_iter=10000;, score=0.321 total time=   0.0s\n",
            "[CV 5/5; 24/30] START C=1, loss=squared_hinge, max_iter=10000...................\n",
            "[CV 5/5; 24/30] END C=1, loss=squared_hinge, max_iter=10000;, score=0.357 total time=   0.0s\n",
            "[CV 1/5; 25/30] START C=10, loss=hinge, max_iter=1000...........................\n",
            "[CV 1/5; 25/30] END C=10, loss=hinge, max_iter=1000;, score=0.448 total time=   0.0s\n",
            "[CV 2/5; 25/30] START C=10, loss=hinge, max_iter=1000...........................\n",
            "[CV 2/5; 25/30] END C=10, loss=hinge, max_iter=1000;, score=0.214 total time=   0.0s\n",
            "[CV 3/5; 25/30] START C=10, loss=hinge, max_iter=1000...........................\n",
            "[CV 3/5; 25/30] END C=10, loss=hinge, max_iter=1000;, score=0.393 total time=   0.0s\n",
            "[CV 4/5; 25/30] START C=10, loss=hinge, max_iter=1000...........................\n",
            "[CV 4/5; 25/30] END C=10, loss=hinge, max_iter=1000;, score=0.321 total time=   0.0s\n",
            "[CV 5/5; 25/30] START C=10, loss=hinge, max_iter=1000...........................\n",
            "[CV 5/5; 25/30] END C=10, loss=hinge, max_iter=1000;, score=0.321 total time=   0.0s\n",
            "[CV 1/5; 26/30] START C=10, loss=hinge, max_iter=5000...........................\n",
            "[CV 1/5; 26/30] END C=10, loss=hinge, max_iter=5000;, score=0.448 total time=   0.0s\n",
            "[CV 2/5; 26/30] START C=10, loss=hinge, max_iter=5000...........................\n",
            "[CV 2/5; 26/30] END C=10, loss=hinge, max_iter=5000;, score=0.214 total time=   0.0s\n",
            "[CV 3/5; 26/30] START C=10, loss=hinge, max_iter=5000...........................\n",
            "[CV 3/5; 26/30] END C=10, loss=hinge, max_iter=5000;, score=0.393 total time=   0.0s\n",
            "[CV 4/5; 26/30] START C=10, loss=hinge, max_iter=5000...........................\n",
            "[CV 4/5; 26/30] END C=10, loss=hinge, max_iter=5000;, score=0.321 total time=   0.0s\n",
            "[CV 5/5; 26/30] START C=10, loss=hinge, max_iter=5000...........................\n",
            "[CV 5/5; 26/30] END C=10, loss=hinge, max_iter=5000;, score=0.321 total time=   0.0s\n",
            "[CV 1/5; 27/30] START C=10, loss=hinge, max_iter=10000..........................\n",
            "[CV 1/5; 27/30] END C=10, loss=hinge, max_iter=10000;, score=0.448 total time=   0.0s\n",
            "[CV 2/5; 27/30] START C=10, loss=hinge, max_iter=10000..........................\n",
            "[CV 2/5; 27/30] END C=10, loss=hinge, max_iter=10000;, score=0.214 total time=   0.0s\n",
            "[CV 3/5; 27/30] START C=10, loss=hinge, max_iter=10000..........................\n",
            "[CV 3/5; 27/30] END C=10, loss=hinge, max_iter=10000;, score=0.393 total time=   0.0s\n",
            "[CV 4/5; 27/30] START C=10, loss=hinge, max_iter=10000..........................\n",
            "[CV 4/5; 27/30] END C=10, loss=hinge, max_iter=10000;, score=0.321 total time=   0.0s\n",
            "[CV 5/5; 27/30] START C=10, loss=hinge, max_iter=10000..........................\n",
            "[CV 5/5; 27/30] END C=10, loss=hinge, max_iter=10000;, score=0.321 total time=   0.0s\n",
            "[CV 1/5; 28/30] START C=10, loss=squared_hinge, max_iter=1000...................\n",
            "[CV 1/5; 28/30] END C=10, loss=squared_hinge, max_iter=1000;, score=0.448 total time=   0.0s\n",
            "[CV 2/5; 28/30] START C=10, loss=squared_hinge, max_iter=1000...................\n",
            "[CV 2/5; 28/30] END C=10, loss=squared_hinge, max_iter=1000;, score=0.214 total time=   0.0s\n",
            "[CV 3/5; 28/30] START C=10, loss=squared_hinge, max_iter=1000...................\n",
            "[CV 3/5; 28/30] END C=10, loss=squared_hinge, max_iter=1000;, score=0.393 total time=   0.1s\n",
            "[CV 4/5; 28/30] START C=10, loss=squared_hinge, max_iter=1000...................\n",
            "[CV 4/5; 28/30] END C=10, loss=squared_hinge, max_iter=1000;, score=0.321 total time=   0.1s\n",
            "[CV 5/5; 28/30] START C=10, loss=squared_hinge, max_iter=1000...................\n",
            "[CV 5/5; 28/30] END C=10, loss=squared_hinge, max_iter=1000;, score=0.321 total time=   0.1s\n",
            "[CV 1/5; 29/30] START C=10, loss=squared_hinge, max_iter=5000...................\n",
            "[CV 1/5; 29/30] END C=10, loss=squared_hinge, max_iter=5000;, score=0.448 total time=   0.0s\n",
            "[CV 2/5; 29/30] START C=10, loss=squared_hinge, max_iter=5000...................\n",
            "[CV 2/5; 29/30] END C=10, loss=squared_hinge, max_iter=5000;, score=0.214 total time=   0.0s\n",
            "[CV 3/5; 29/30] START C=10, loss=squared_hinge, max_iter=5000...................\n",
            "[CV 3/5; 29/30] END C=10, loss=squared_hinge, max_iter=5000;, score=0.393 total time=   0.1s\n",
            "[CV 4/5; 29/30] START C=10, loss=squared_hinge, max_iter=5000...................\n",
            "[CV 4/5; 29/30] END C=10, loss=squared_hinge, max_iter=5000;, score=0.321 total time=   0.1s\n",
            "[CV 5/5; 29/30] START C=10, loss=squared_hinge, max_iter=5000...................\n",
            "[CV 5/5; 29/30] END C=10, loss=squared_hinge, max_iter=5000;, score=0.321 total time=   0.1s\n",
            "[CV 1/5; 30/30] START C=10, loss=squared_hinge, max_iter=10000..................\n",
            "[CV 1/5; 30/30] END C=10, loss=squared_hinge, max_iter=10000;, score=0.448 total time=   0.0s\n",
            "[CV 2/5; 30/30] START C=10, loss=squared_hinge, max_iter=10000..................\n",
            "[CV 2/5; 30/30] END C=10, loss=squared_hinge, max_iter=10000;, score=0.214 total time=   0.0s\n",
            "[CV 3/5; 30/30] START C=10, loss=squared_hinge, max_iter=10000..................\n",
            "[CV 3/5; 30/30] END C=10, loss=squared_hinge, max_iter=10000;, score=0.393 total time=   0.1s\n",
            "[CV 4/5; 30/30] START C=10, loss=squared_hinge, max_iter=10000..................\n",
            "[CV 4/5; 30/30] END C=10, loss=squared_hinge, max_iter=10000;, score=0.321 total time=   0.1s\n",
            "[CV 5/5; 30/30] START C=10, loss=squared_hinge, max_iter=10000..................\n",
            "[CV 5/5; 30/30] END C=10, loss=squared_hinge, max_iter=10000;, score=0.321 total time=   0.1s\n",
            "Best parameters for LinearSVC: {'C': 10, 'loss': 'hinge', 'max_iter': 1000}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 5 random records from the DataFrame\n",
        "random_records = df.sample(n=5)"
      ],
      "metadata": {
        "id": "6NrYUA6HrF2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the lyrics of the sampled records\n",
        "lyrics_vec = vectorizer.transform(random_records['Lyrics'])\n",
        "\n",
        "# Predict labels for the vectorized lyrics\n",
        "predicted_labels = grid_search_svc.predict(lyrics_vec)\n",
        "\n",
        "# Print song title, artist, and the beginning of the lyrics along with predicted labels\n",
        "for index, (title, artist, lyrics, label) in enumerate(zip(random_records['Song Name'], random_records['Artist'], random_records['Lyrics'], predicted_labels)):\n",
        "    print(f\"Record {index+1}:\")\n",
        "    print(f\"Song Title: {title}\")\n",
        "    print(f\"Artist: {artist}\")\n",
        "    print(f\"Lyrics: {lyrics[:100]}...\")  # Print only the beginning of the lyrics (first 50 characters)\n",
        "    print(f\"Predicted Label: {label}\\n\")\n"
      ],
      "metadata": {
        "id": "MOMUKMv634-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14c7375d-42f6-471d-f6fb-7983db773ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Record 1:\n",
            "Song Title: Delilah\n",
            "Artist: Tom Jones\n",
            "Lyrics: I see light night I pass window I see flicker shadow love blind woman deceive I I watch go mind deli...\n",
            "Predicted Label: Heartbreak\n",
            "\n",
            "Record 2:\n",
            "Song Title: Big Yellow Taxi\n",
            "Artist: Joni Mitchell\n",
            "Lyrics: pave paradise put parking lot pink hotel boutique swinge hot spot always seem go know get till go pa...\n",
            "Predicted Label: Protest\n",
            "\n",
            "Record 3:\n",
            "Song Title: Personal Jesus\n",
            "Artist: Depeche Mode\n",
            "Lyrics: personal jesus someone hear prayer someone care personal jesus someone hear prayer someone feel unkn...\n",
            "Predicted Label: Sex\n",
            "\n",
            "Record 4:\n",
            "Song Title: Martha\n",
            "Artist: Tom Waits\n",
            "Lyrics: operator number please many year remember old voice I fight tear hello hello martha old tom frost I ...\n",
            "Predicted Label: Life and death\n",
            "\n",
            "Record 5:\n",
            "Song Title: Crazy For You\n",
            "Artist: Madonna\n",
            "Lyrics: swaying room music start stranger make dark two two body become one I see smokey air feel weight sta...\n",
            "Predicted Label: Heartbreak\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now examine your results, look at some individual observations and investigate whether the model predictions are logical/appear accurate. Describe your findings below:**"
      ],
      "metadata": {
        "id": "fjT7zU_j3_MN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's not bad. With a bigger dataset, I'm sure that the outcome could be better. Overall however, it's doing the job and categorizing the songs fairly well."
      ],
      "metadata": {
        "id": "ToUv8-y14LNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now select a pretrained model from Hugging Face (linked above) and make predictions onto your Unit 1 dataset. Compare how it appears to perform against how the model you trained appeared to perform.**"
      ],
      "metadata": {
        "id": "A6yZIFQm4xJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_predictions = df.sample(n=50) # use a subset\n",
        "\n",
        "# Make a copy of the DataFrame to avoid modifying the original one\n",
        "df_with_predictions = df.copy()\n",
        "\n",
        "# Initialize the zero-shot classification pipeline with a pre-trained model\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Define possible themes\n",
        "possible_themes = [\"Heartbreak\", \"People and places\", \"Life and death\", \"Love\", \"Sex\", \"Party songs\", \"Protest\"]\n",
        "\n",
        "# Create an empty list to store predictions\n",
        "predictions = []\n",
        "\n",
        "# Predict labels for each row in the DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    # Generate classification with the lyrics\n",
        "    result = classifier(row['Lyrics'], possible_themes)\n",
        "\n",
        "    # Get the most likely theme\n",
        "    predicted_theme = result['labels'][0]\n",
        "\n",
        "    # Append prediction to the list\n",
        "    predictions.append(predicted_theme)\n",
        "\n",
        "# Add predictions as a new column to the DataFrame\n",
        "df_with_predictions['Predicted Theme'] = predictions\n",
        "\n",
        "# Display the DataFrame with predictions\n",
        "print(\"DataFrame with Predictions:\")\n",
        "print(df_with_predictions)"
      ],
      "metadata": {
        "id": "s3b0clwx49zi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecfdd01b-17d1-4d96-ad15-3b86e6d095f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for index, (title, artist, lyrics) in enumerate(zip(random_records['Song Name'], random_records['Artist'], random_records['Lyrics'])):\n",
        "    # Generate classification with the lyrics\n",
        "    result = classifier(lyrics, possible_themes)\n",
        "\n",
        "    # Get the most likely theme\n",
        "    predicted_theme = result['labels'][0]\n",
        "\n",
        "    # Print song title, artist, and the beginning of the lyrics along with predicted label\n",
        "    print(f\"Record {index+1}:\")\n",
        "    print(f\"Song Title: {title}\")\n",
        "    print(f\"Artist: {artist}\")\n",
        "    print(f\"Lyrics: {lyrics[:50]}...\")  # Print only the beginning of the lyrics (first 50 characters)\n",
        "    print(f\"Predicted Theme: {predicted_theme}\\n\")"
      ],
      "metadata": {
        "id": "1cDPTGRQuL91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hM4KEiF949Fy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How could you incorporate supervised classification (document or sentiment classification) into a product? -- think about what it could be useful for as we continue to work towards your final project.**"
      ],
      "metadata": {
        "id": "4zkcXV855Ixi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eycly46R5XLC"
      }
    }
  ]
}